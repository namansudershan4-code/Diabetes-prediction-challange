{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe176a0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-13T13:12:20.808145Z",
     "iopub.status.busy": "2025-12-13T13:12:20.807736Z",
     "iopub.status.idle": "2025-12-13T13:12:22.903778Z",
     "shell.execute_reply": "2025-12-13T13:12:22.902720Z"
    },
    "papermill": {
     "duration": 2.101827,
     "end_time": "2025-12-13T13:12:22.905758",
     "exception": false,
     "start_time": "2025-12-13T13:12:20.803931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/playground-series-s5e12/sample_submission.csv\n",
      "/kaggle/input/playground-series-s5e12/train.csv\n",
      "/kaggle/input/playground-series-s5e12/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d2a155",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:12:22.912262Z",
     "iopub.status.busy": "2025-12-13T13:12:22.911789Z",
     "iopub.status.idle": "2025-12-13T13:39:25.982571Z",
     "shell.execute_reply": "2025-12-13T13:39:25.981642Z"
    },
    "papermill": {
     "duration": 1623.075238,
     "end_time": "2025-12-13T13:39:25.983960",
     "exception": false,
     "start_time": "2025-12-13T13:12:22.908722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train final shape: (700000, 41)\n",
      "Test final shape : (300000, 41)\n",
      "Using best xgb params: {'n_estimators': 2000, 'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "\n",
      "========== FOLD 1 ==========\n",
      "XGB Fold AUC: 0.7273424257258376\n",
      "[LightGBM] [Info] Number of positive: 349045, number of negative: 210955\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2927\n",
      "[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623295 -> initscore=0.503556\n",
      "[LightGBM] [Info] Start training from score 0.503556\n",
      "LGB Fold AUC: 0.7265178849300913\n",
      "CAT Fold AUC: 0.7277981155035234\n",
      "\n",
      "========== FOLD 2 ==========\n",
      "XGB Fold AUC: 0.7255888559640764\n",
      "[LightGBM] [Info] Number of positive: 349045, number of negative: 210955\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2928\n",
      "[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623295 -> initscore=0.503556\n",
      "[LightGBM] [Info] Start training from score 0.503556\n",
      "LGB Fold AUC: 0.724603192561459\n",
      "CAT Fold AUC: 0.7260911006988813\n",
      "\n",
      "========== FOLD 3 ==========\n",
      "XGB Fold AUC: 0.7261323729214229\n",
      "[LightGBM] [Info] Number of positive: 349046, number of negative: 210954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2930\n",
      "[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623296 -> initscore=0.503564\n",
      "[LightGBM] [Info] Start training from score 0.503564\n",
      "LGB Fold AUC: 0.7255260402603902\n",
      "CAT Fold AUC: 0.726625254814619\n",
      "\n",
      "========== FOLD 4 ==========\n",
      "XGB Fold AUC: 0.7277844875188282\n",
      "[LightGBM] [Info] Number of positive: 349046, number of negative: 210954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2927\n",
      "[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623296 -> initscore=0.503564\n",
      "[LightGBM] [Info] Start training from score 0.503564\n",
      "LGB Fold AUC: 0.726446969573196\n",
      "CAT Fold AUC: 0.7279114594116994\n",
      "\n",
      "========== FOLD 5 ==========\n",
      "XGB Fold AUC: 0.7264987969309283\n",
      "[LightGBM] [Info] Number of positive: 349046, number of negative: 210954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2926\n",
      "[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623296 -> initscore=0.503564\n",
      "[LightGBM] [Info] Start training from score 0.503564\n",
      "LGB Fold AUC: 0.7264376420068924\n",
      "CAT Fold AUC: 0.7271783069636618\n",
      "\n",
      "===== OOF SCORES =====\n",
      "XGB OOF AUC: 0.7266652072132228\n",
      "LGB OOF AUC: 0.7259038522964348\n",
      "CAT OOF AUC: 0.7271182565209366\n",
      "\n",
      "Saved stacking_submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosed_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>700000</td>\n",
       "      <td>0.500515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>700001</td>\n",
       "      <td>0.730713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>700002</td>\n",
       "      <td>0.802217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>700003</td>\n",
       "      <td>0.386690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>700004</td>\n",
       "      <td>0.888286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  diagnosed_diabetes\n",
       "0  700000            0.500515\n",
       "1  700001            0.730713\n",
       "2  700002            0.802217\n",
       "3  700003            0.386690\n",
       "4  700004            0.888286"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------\n",
    "# LOAD DATA\n",
    "# -------------------------\n",
    "dataTrain = pd.read_csv(\"/kaggle/input/playground-series-s5e12/train.csv\")\n",
    "dataTest  = pd.read_csv(\"/kaggle/input/playground-series-s5e12/test.csv\")\n",
    "\n",
    "dfTrain = pd.DataFrame(dataTrain)\n",
    "dfTest  = pd.DataFrame(dataTest)\n",
    "\n",
    "# -------------------------\n",
    "# FEATURE ENGINEERING (your original features)\n",
    "# -------------------------\n",
    "dfTrain[\"age_bmi\"] = dfTrain[\"age\"] * dfTrain[\"bmi\"]\n",
    "dfTrain[\"waist_hip_ratio&age\"] = dfTrain[\"waist_to_hip_ratio\"] * dfTrain[\"age\"]\n",
    "dfTrain[\"bmi_waist\"] = dfTrain[\"waist_to_hip_ratio\"] * dfTrain[\"bmi\"]\n",
    "dfTrain[\"age_bmi_waist\"] = dfTrain[\"age\"] * dfTrain[\"bmi\"] * dfTrain[\"waist_to_hip_ratio\"]\n",
    "\n",
    "dfTest[\"age_bmi\"] = dfTest[\"age\"] * dfTest[\"bmi\"]\n",
    "dfTest[\"waist_hip_ratio&age\"] = dfTest[\"waist_to_hip_ratio\"] * dfTest[\"age\"]\n",
    "dfTest[\"bmi_waist\"] = dfTest[\"waist_to_hip_ratio\"] * dfTest[\"bmi\"]\n",
    "dfTest[\"age_bmi_waist\"] = dfTest[\"age\"] * dfTest[\"bmi\"] * dfTest[\"waist_to_hip_ratio\"]\n",
    "\n",
    "# -------------------------\n",
    "# Separate categorical and numerical columns\n",
    "# -------------------------\n",
    "# Note: ensure the true target column name is 'diagnosed_diabetes' as in your original code\n",
    "TARGET = \"diagnosed_diabetes\"\n",
    "\n",
    "cat_cols = dfTrain.select_dtypes(exclude=np.number).columns.tolist()\n",
    "# If there are no categorical cols this will be empty; OneHotEncoder will handle it.\n",
    "num_cols = dfTrain.select_dtypes(include=np.number).columns.drop(TARGET).tolist()\n",
    "\n",
    "cat_train = dfTrain[cat_cols]\n",
    "cat_test  = dfTest[cat_cols]\n",
    "\n",
    "num_train = dfTrain[num_cols]\n",
    "num_test  = dfTest[num_cols]\n",
    "\n",
    "# -------------------------\n",
    "# ONE-HOT ENCODING (your original approach)\n",
    "# -------------------------\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# older sklearn uses sparse=False; using that for broad compatibility\n",
    "ohe = OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse=False)\n",
    "if len(cat_cols) > 0:\n",
    "    X_cat = ohe.fit_transform(cat_train)\n",
    "    Y_cat = ohe.transform(cat_test)\n",
    "else:\n",
    "    # no categorical columns -> create empty arrays with correct number of rows\n",
    "    X_cat = np.zeros((len(dfTrain), 0))\n",
    "    Y_cat = np.zeros((len(dfTest), 0))\n",
    "\n",
    "# combine OHE output with numeric arrays\n",
    "dfTrain_final = np.hstack([X_cat, num_train.to_numpy()])\n",
    "dfTest_final  = np.hstack([Y_cat,  num_test.to_numpy()])\n",
    "\n",
    "# Quick check shapes\n",
    "print(\"Train final shape:\", dfTrain_final.shape)\n",
    "print(\"Test final shape :\", dfTest_final.shape)\n",
    "\n",
    "# -------------------------\n",
    "# Prepare data variables used by stacking\n",
    "# -------------------------\n",
    "X = dfTrain_final            # numpy array\n",
    "y = dfTrain[TARGET].values   # 1D numpy array\n",
    "X_test = dfTest_final        # numpy array\n",
    "\n",
    "# -------------------------\n",
    "# Imports for stacking models\n",
    "# -------------------------\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# -------------------------\n",
    "# BEST XGB PARAMS (manually provided by you)\n",
    "# -------------------------\n",
    "best_params = {\n",
    "    \"n_estimators\": 2000,\n",
    "    \"max_depth\": 4,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8\n",
    "}\n",
    "print(\"Using best xgb params:\", best_params)\n",
    "\n",
    "# -------------------------\n",
    "# Prepare OOF and test prediction containers\n",
    "# -------------------------\n",
    "NFOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_xgb = np.zeros(len(X), dtype=float)\n",
    "oof_lgb = np.zeros(len(X), dtype=float)\n",
    "oof_cat = np.zeros(len(X), dtype=float)\n",
    "\n",
    "test_pred_xgb = np.zeros(len(X_test), dtype=float)\n",
    "test_pred_lgb = np.zeros(len(X_test), dtype=float)\n",
    "test_pred_cat = np.zeros(len(X_test), dtype=float)\n",
    "\n",
    "# -------------------------\n",
    "# K-FOLD TRAINING LOOP\n",
    "# -------------------------\n",
    "fold = 1\n",
    "for train_idx, val_idx in skf.split(X, y):\n",
    "    print(f\"\\n========== FOLD {fold} ==========\")\n",
    "    # select rows (X is numpy array)\n",
    "    X_tr, X_val = X[train_idx], X[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # ---- XGBoost\n",
    "    xgb_model = XGBClassifier(\n",
    "        n_estimators=best_params[\"n_estimators\"],\n",
    "        max_depth=best_params[\"max_depth\"],\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        subsample=best_params[\"subsample\"],\n",
    "        colsample_bytree=best_params[\"colsample_bytree\"],\n",
    "        eval_metric=\"auc\",\n",
    "        use_label_encoder=False,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    xgb_model.fit(X_tr, y_tr)\n",
    "    oof_xgb[val_idx] = xgb_model.predict_proba(X_val)[:, 1]\n",
    "    test_pred_xgb += xgb_model.predict_proba(X_test)[:, 1] / NFOLDS\n",
    "    print(\"XGB Fold AUC:\", roc_auc_score(y_val, oof_xgb[val_idx]))\n",
    "\n",
    "    # ---- LightGBM\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.03,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.7,\n",
    "        max_depth=-1,\n",
    "        random_state=42,\n",
    "        objective=\"binary\"\n",
    "    )\n",
    "    lgb_model.fit(X_tr, y_tr)\n",
    "    oof_lgb[val_idx] = lgb_model.predict_proba(X_val)[:, 1]\n",
    "    test_pred_lgb += lgb_model.predict_proba(X_test)[:, 1] / NFOLDS\n",
    "    print(\"LGB Fold AUC:\", roc_auc_score(y_val, oof_lgb[val_idx]))\n",
    "\n",
    "    # ---- CatBoost\n",
    "    cat_model = CatBoostClassifier(\n",
    "        iterations=2000,\n",
    "        learning_rate=0.07,\n",
    "        l2_leaf_reg=1,\n",
    "        depth=6,\n",
    "        loss_function=\"Logloss\",\n",
    "        verbose=False,\n",
    "        random_state=42\n",
    "    )\n",
    "    cat_model.fit(X_tr, y_tr)\n",
    "    oof_cat[val_idx] = cat_model.predict_proba(X_val)[:, 1]\n",
    "    test_pred_cat += cat_model.predict_proba(X_test)[:, 1] / NFOLDS\n",
    "    print(\"CAT Fold AUC:\", roc_auc_score(y_val, oof_cat[val_idx]))\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# -------------------------\n",
    "# Print OOF performance for each base model\n",
    "# -------------------------\n",
    "print(\"\\n===== OOF SCORES =====\")\n",
    "print(\"XGB OOF AUC:\", roc_auc_score(y, oof_xgb))\n",
    "print(\"LGB OOF AUC:\", roc_auc_score(y, oof_lgb))\n",
    "print(\"CAT OOF AUC:\", roc_auc_score(y, oof_cat))\n",
    "\n",
    "# -------------------------\n",
    "# META-MODEL TRAINING (stacking)\n",
    "# -------------------------\n",
    "meta_train = np.vstack([oof_xgb, oof_lgb, oof_cat]).T\n",
    "meta_test  = np.vstack([test_pred_xgb, test_pred_lgb, test_pred_cat]).T\n",
    "\n",
    "meta_model = LogisticRegression(max_iter=1000, solver=\"lbfgs\")\n",
    "meta_model.fit(meta_train, y)\n",
    "\n",
    "stacked_pred_test = meta_model.predict_proba(meta_test)[:, 1]\n",
    "\n",
    "# -------------------------\n",
    "# Create submission\n",
    "# -------------------------\n",
    "submission = pd.read_csv(\"/kaggle/input/playground-series-s5e12/sample_submission.csv\")\n",
    "# Ensure sample_submission has 'id' column; use same name as file if different adjust below\n",
    "submission[\"diagnosed_diabetes\"] = stacked_pred_test\n",
    "submission = submission[[\"id\", \"diagnosed_diabetes\"]]\n",
    "submission.to_csv(\"stacking_submission.csv\", index=False)\n",
    "print(\"\\nSaved stacking_submission.csv\")\n",
    "display(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd556848",
   "metadata": {
    "papermill": {
     "duration": 0.00331,
     "end_time": "2025-12-13T13:39:25.991138",
     "exception": false,
     "start_time": "2025-12-13T13:39:25.987828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14272474,
     "sourceId": 91723,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1631.376062,
   "end_time": "2025-12-13T13:39:26.915552",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-13T13:12:15.539490",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
